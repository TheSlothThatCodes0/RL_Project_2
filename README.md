# Deep Reinforcement Learning Applied to a Robotic Pick-and-Place Application

## 1. Project Overview

This project implements and evaluates deep reinforcement learning (DRL) algorithms for a robotic pick-and-place application. The primary objective is to develop an end-to-end learning framework that enables a robotic manipulator to grasp and lift objects using visual feedback from a single RGB-D camera.

The project contrasts two distinct reinforcement learning approaches:

1. **Visual Deep Q-Network (DQN):** A custom implementation that processes RGB-D images to generate pixel-wise Q-maps for grasping control. This approach operates under partial observability, relying solely on visual data.
2. **Proximal Policy Optimization (PPO):** A baseline implementation utilizing full state information (joint positions, object coordinates) to benchmark performance.

## 2. Motivation

Traditional robotic manipulation often relies on hardcoded algorithms that struggle in unstructured environments with varying object poses, lighting conditions, or clutter. Deep Reinforcement Learning offers a robust alternative by learning grasping strategies directly from interactions, thereby eliminating the need for manual feature engineering or precise environmental calibration. Furthermore, the use of a single front-facing RGB-D camera presents a cost-effective and simplified hardware setup compared to multi-sensor arrays.

## 3. Methodology

### 3.1 Simulation Environment

The project is built upon **NVIDIA Isaac Lab**, leveraging its capabilities for parallelized simulation to accelerate training and data generation. The simulation environment is configured to replicate a real-world setup involving an OpenManipulator-X robotic arm.

### 3.2 Robotic Platform

* **Robot:** OpenManipulator-X (4 Degrees of Freedom).
* **Sensors:** Simulated RGB-D camera providing color and depth streams.
* **Task:** The specific task involves the robot identifying, reaching for, grasping, and lifting a target object (e.g., a cube) from a table.

### 3.3 Algorithms Implemented

#### Visual DQN Agent

This custom agent allows the robot to learn optimal grasping policies directly from high-dimensional visual inputs.

* **Observation Space:** RGB and Depth images captured from the simulated camera.
* **Action Space:** Discrete actions inferred from the pixel-wise Q-values generated by the network.
* **Implementation Location:** `robotis_lab/source/robotis_lab/robotis_lab/simulation_tasks/manager_based/open_manipulator_x/lift_visual_dqn`.

#### PPO Agent

A standard PPO implementation serves as the performance baseline, benefiting from access to the complete state of the environment.

* **Observation Space:** Full state vector including joint angles, end-effector position, and target object coordinates.
* **Action Space:** Continuous control of the robot joints.
* **Implementation Location:** `robotis_lab/source/robotis_lab/robotis_lab/simulation_tasks/manager_based/open_manipulator_x/lift`.

## 4. Reward Architecture

The reward function is designed to guide the agent through the sequential phases of the manipulation task: reaching the object, lifting the object, and maintaining a stable grasp. The total reward is a weighted sum of individual components:



The components are defined as follows:

* **Reach Reward:** Incentivizes the end-effector to minimize the distance to the target object.

* **Lift Reward:** Granted when the object is successfully raised above a specific height threshold while maintaining proximity to the end-effector.

* **Grasp Reward:** Encourages the gripper to close firmly around the object.


## 5. Results and Observations

Comparative analysis indicates that the PPO agent, utilizing full environmental state knowledge (oracle baseline), outperforms the Visual DQN implementation. This discrepancy highlights the challenge of inferring precise spatial coordinates and manipulation strategies solely from partial visual observations (single camera RGB-D feed) compared to having direct access to object ground-truth data.

## 6. Contributors

**Group Members (Indraprastha Institute of Information Technology, Delhi):**

* Siddharth Yadav
* Parth Goyal
* Jil Sharma
* Ayush Singh

## 7. References

* Gomes, Natanael & Martins, Felipe & Lima, José & Wörtche, Heinrich. (2021). Deep Reinforcement Learning Applied to a Robotic Pick-and-Place Application. *Robotics in Smart Manufacturing*, 10.1007/978-3-030-91885-9_18.
* Isaac Lab: NVIDIA's modular framework for robot learning.
